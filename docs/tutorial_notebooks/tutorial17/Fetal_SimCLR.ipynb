{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siyusa/Contrastive-Learning/blob/main/docs/tutorial_notebooks/tutorial17/SimCLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Contrastive Learning with SimCLR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9ovhnmY6xvj",
        "outputId": "127cea7b-2b82-45c1-9a40-c4c28200bad8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n",
            "Number of workers: 16\n"
          ]
        }
      ],
      "source": [
        "#os 模块提供了与操作系统交互的功能，如读取环境变量、管理文件和目录等\n",
        "import os\n",
        "# deepcopy 用于创建对象的深拷贝，即完全复制一个对象及其所有嵌套的内容，修改新对象不会影响原对象。\n",
        "from copy import deepcopy\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "# 将 matplotlib 的默认色彩映射设置为 ‘cividis’。这是一种现代、感知均匀、对色盲友好的颜色方案，常用于科学出版物，能更好地呈现数据细节\n",
        "plt.set_cmap('cividis')\n",
        "# 这是一个 IPython 魔法命令（以 % 开头）。使 matplotlib 的图形能够直接嵌入在 Jupyter Notebook 中显示，而不是作为单独的窗口弹出\n",
        "%matplotlib inline\n",
        "# 设置 matplotlib 的输出格式为 SVG 和 PDF，这两种格式适合高质量的图形导出和打印\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "# 将绘图中所有线条的默认宽度设置为 2.0。这使得图表中的线条比默认值更粗、更清晰\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "# seaborn 是一个基于 matplotlib 的高级统计图形库和数据可视化库，提供更高级的接口和美观的默认样式。下面的代码导入 seaborn 并应用其默认样式设置\n",
        "import seaborn as sns\n",
        "# 激活 seaborn 的默认主题和样式设置，自动覆盖 matplotlib 原有的样式，使图表立即变得更具现代感和美观性（如更改背景、网格、字体、调色板等）\n",
        "sns.set()\n",
        "\n",
        "## tqdm for loading bars\n",
        "# tqdm 是一个用于显示循环进度条的 Python 库，特别适合在长时间运行的任务中提供视觉反馈。下面的代码导入 tqdm 的 notebook 版本，以便在 Jupyter Notebook 环境中使用\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "# Torch 的神经网络模块，包含各种神经网络层、损失函数等\n",
        "import torch.nn as nn\n",
        "# Torch 的函数式接口，提供各种操作的无状态版本，如激活函数、卷积操作等\n",
        "import torch.nn.functional as F\n",
        "# Torch 的数据处理模块，包含数据集和数据加载器等\n",
        "import torch.utils.data as data\n",
        "# Torch 的优化器模块，包含各种优化算法，如 SGD、Adam 等\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "# Torchvision 是 PyTorch 的一个子库，专注于计算机视觉任务，提供了常用的数据集、模型和图像变换工具\n",
        "import torchvision\n",
        "# torchvision 的数据集模块，包含常用的计算机视觉数据集，如 CIFAR10、ImageNet 等\n",
        "from torchvision.datasets import STL10\n",
        "# torchvision 的图像变换模块，提供各种图像预处理和数据增强操作\n",
        "from torchvision import transforms\n",
        "\n",
        "# PyTorch Lightning\n",
        "# PyTorch Lightning 是一个用于简化 PyTorch 代码结构和训练流程的高层框架，旨在提高代码的可读性和可维护性，同时支持分布式训练等高级功能\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "# PyTorch Lightning 的回调模块，包含各种训练过程中的回调函数，如学习率监控、模型检查点等\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Import tensorboard\n",
        "# TensorBoard 是一个用于可视化机器学习实验的工具，常与 TensorFlow 和 PyTorch 一起使用。下面的代码导入 TensorBoard 的扩展，以便在 Jupyter Notebook 中使用\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial17\"\n",
        "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
        "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Number of workers:\", NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rYPD42o6xvk"
      },
      "outputs": [],
      "source": [
        "contrast_transforms = transforms.Compose([transforms.RandomResizedCrop(size=256),\n",
        "                                          transforms.RandomApply([\n",
        "                                              transforms.ColorJitter(brightness=0.5,\n",
        "                                                                     contrast=0.5,\n",
        "                                                                     saturation=0.5,\n",
        "                                                                     hue=0.1)\n",
        "                                          ], p=0.8),\n",
        "                                          transforms.RandomGrayscale(p=0.2),\n",
        "                                          transforms.RandomHorizontalFlip(),\n",
        "                                          transforms.GaussianBlur(kernel_size=9),\n",
        "                                         ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxU4LA0-6xvl"
      },
      "outputs": [],
      "source": [
        "class SimCLRUSDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads images under directory structure:\n",
        "    data/<plane>/*.png\n",
        "    Returns:\n",
        "      x1, x2 : two augmented views for SimCLR (tensor [3,H,W], values in [0,1])\n",
        "      enc   : weakly augmented masked image for encoder input ([1,H,W], values in [0,1])\n",
        "      path  : original path\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir='./data', size=512, data_transforms=None, n_views=2):\n",
        "        self.root = Path(root_dir)\n",
        "        exts = ('.png','.jpg','.jpeg','.bmp','.tiff','.tif')\n",
        "        # 递归查找self.root目录及其所有子目录中，扩展名在exts集合中的所有文件，并按字母顺序返回它们的路径列表。\n",
        "        self.samples = [p for p in sorted(self.root.rglob('*')) if p.suffix.lower() in exts] \n",
        "        self.size = size\n",
        "\n",
        "        # Strong augmentations for SimCLR views\n",
        "        self.base_transforms = data_transforms\n",
        "        self.n_views = n_views\n",
        "        self.augment2 = T.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "                                                  \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.samples[idx]\n",
        "        img = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            raise RuntimeError(f\"Cannot read image: {p}\")\n",
        "\n",
        "        \"\"\"\n",
        "        x1 = self.base_transforms(img)  # [3,H,W], \n",
        "        x2 = self.base_transforms(img)\n",
        "\n",
        "        plt.figure(figsize=(10,15))\n",
        "        plt.title('Augmented image examples of the dataset')\n",
        "        plt.subplot(1,3,1)\n",
        "        plt.imshow(img)\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.imshow(x1.transpose(0,1).transpose(1,2))  # (c,h,w) to (h,w,c)\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.imshow(x2.transpose(0,1).transpose(1,2))  \n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        \"\"\"  \n",
        "\n",
        "        return [self.base_transforms(img) for i in range(self.n_views)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEGgMAxx6xvm"
      },
      "source": [
        "Finally, before starting with our implementation of SimCLR, let's look at some example image pairs sampled with our augmentations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIwX5GT56xvm"
      },
      "source": [
        "We see the wide variety of our data augmentation, including randomly cropping, grayscaling, gaussian blur, and color distortion. Thus, it remains a challenging task for the model to match two, independently augmented patches of the same image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb2bdYg86xvn"
      },
      "source": [
        "### SimCLR implementation\n",
        "\n",
        "Using the data loader pipeline above, we can now implement SimCLR. At each iteration, we get for every image $x$ two differently augmented versions, which we refer to as $\\tilde{x}_i$ and $\\tilde{x}_j$. Both of these images are encoded into a one-dimensional feature vector, between which we want to maximize similarity which minimizes it to all other images in the batch. The encoder network is split into two parts: a base encoder network $f(\\cdot)$, and a projection head $g(\\cdot)$. The base network is usually a deep CNN as we have seen in e.g. [Tutorial 5](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html) before, and is responsible for extracting a representation vector from the augmented data examples. In our experiments, we will use the common ResNet-18 architecture as $f(\\cdot)$, and refer to the output as $f(\\tilde{x}_i)=h_i$. The projection head $g(\\cdot)$ maps the representation $h$ into a space where we apply the contrastive loss, i.e., compare similarities between vectors. It is often chosen to be a small MLP with non-linearities, and for simplicity, we follow the original SimCLR paper setup by defining it as a two-layer MLP with ReLU activation in the hidden layer. Note that in the follow-up paper, [SimCLRv2](https://arxiv.org/abs/2006.10029), the authors mention that larger/wider MLPs can boost the performance considerably. This is why we apply an MLP with four times larger hidden dimensions, but deeper MLPs showed to overfit on the given dataset. The general setup is visualized below (figure credit - [Ting Chen et al.](https://arxiv.org/abs/2006.10029)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/simclr_network_setup.svg?raw=1\" width=\"350px\"></center>\n",
        "\n",
        "After finishing the training with contrastive learning, we will remove the projection head $g(\\cdot)$, and use $f(\\cdot)$ as a pretrained feature extractor. The representations $z$ that come out of the projection head $g(\\cdot)$ have been shown to perform worse than those of the base network $f(\\cdot)$ when finetuning the network for a new task. This is likely because the representations $z$ are trained to become invariant to many features like the color that can be important for downstream tasks. Thus, $g(\\cdot)$ is only needed for the contrastive learning stage.\n",
        "\n",
        "Now that the architecture is described, let's take a closer look at how we train the model. As mentioned before, we want to maximize the similarity between the representations of the two augmented versions of the same image, i.e., $z_i$ and $z_j$ in the figure above, while minimizing it to all other examples in the batch. SimCLR thereby applies the InfoNCE loss, originally proposed by [Aaron van den Oord et al.](https://arxiv.org/abs/1807.03748) for contrastive learning. In short, the InfoNCE loss compares the similarity of $z_i$ and $z_j$ to the similarity of $z_i$ to any other representation in the batch by performing a softmax over the similarity values. The loss can be formally written as:\n",
        "\n",
        "$$\n",
        "\\ell_{i,j}=-\\log \\frac{\\exp(\\text{sim}(z_i,z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)}=-\\text{sim}(z_i,z_j)/\\tau+\\log\\left[\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i,z_k)/\\tau)\\right]\n",
        "$$\n",
        "\n",
        "The function $\\text{sim}$ is a similarity metric, and the hyperparameter $\\tau$ is called temperature determining how peaked the distribution is. Since many similarity metrics are bounded, the temperature parameter allows us to balance the influence of many dissimilar image patches versus one similar patch. The similarity metric that is used in SimCLR is cosine similarity, as defined below:\n",
        "\n",
        "$$\n",
        "\\text{sim}(z_i,z_j) = \\frac{z_i^\\top \\cdot z_j}{||z_i||\\cdot||z_j||}\n",
        "$$\n",
        "\n",
        "The maximum cosine similarity possible is $1$, while the minimum is $-1$. In general, we will see that the features of two different images will converge to a cosine similarity around zero since the minimum, $-1$, would require $z_i$ and $z_j$ to be in the exact opposite direction in all feature dimensions, which does not allow for great flexibility.\n",
        "\n",
        "Finally, now that we have discussed all details, let's implement SimCLR below as a PyTorch Lightning module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldIgJtUH6xvn"
      },
      "outputs": [],
      "source": [
        "class SimCLR(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
        "        super().__init__()\n",
        "        # 保存所有传入的超参数到 self.hparams 字典中，便于日志记录和模型检查点保存\n",
        "        self.save_hyperparameters()\n",
        "        # 断言检查，确保温度参数为正数（负温度无意义）\n",
        "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
        "        # 加载预训练的 ResNet18 模型，将最后全连接层输出改为 4*hidden_dim，作为Base model f(.) \n",
        "        self.convnet = torchvision.models.resnet18(num_classes=4*hidden_dim)  # Output of last linear layer\n",
        "        # The MLP for g(.) consists of Linear->ReLU->Linear\n",
        "        self.convnet.fc = nn.Sequential(\n",
        "            self.convnet.fc,  # Linear(ResNet output, 4*hidden_dim)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4*hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        #  配置优化器和学习率调度器\n",
        "        optimizer = optim.AdamW(self.parameters(),\n",
        "                                lr=self.hparams.lr,\n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                            T_max=self.hparams.max_epochs,\n",
        "                                                            eta_min=self.hparams.lr/50)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def info_nce_loss(self, batch, mode='train'):\n",
        "        imgs, _ = batch   #imgs 的原始shape：类型：元组，包含2个张量, 每个张量shape：[batch_size, channels, height, width]\n",
        "        imgs = torch.cat(imgs, dim=0) # [batch_size, C, H, W] × 2 → [2*batch_size, C, H, W]\n",
        "\n",
        "        # Encode all images\n",
        "        feats = self.convnet(imgs)   # feats: [2*batch_size, hidden_dim]\n",
        "        # Calculate cosine similarity 计算所有特征对之间的余弦相似度矩阵\n",
        "        # feats[:,None,:]: 形状 [2N, 1, D]; feats[None,:,:]: 形状 [1, 2N, D]; 结果: [2N, 2N] 相似度矩阵\n",
        "        cos_sim = F.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1)\n",
        "        # Mask out cosine similarity to itself 创建对角线掩码并填充极大负值. 排除每个样本与自身的相似度（softmax 中 exp(-9e15) ≈ 0）\n",
        "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
        "        cos_sim.masked_fill_(self_mask, -9e15)\n",
        "        # Find positive example -> batch_size//2 away from the original example\n",
        "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
        "        # InfoNCE loss\n",
        "        cos_sim = cos_sim / self.hparams.temperature\n",
        "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1) # InfoNCE 损失,l(i,j) = -log[exp(sim(zᵢ,zⱼ)/τ) / ∑ exp(sim(zᵢ,zₖ)/τ)]\n",
        "        nll = nll.mean()\n",
        "\n",
        "        # Logging loss\n",
        "        self.log(mode+'_loss', nll)\n",
        "        # 构建用于排名的相似度矩阵 Get ranking position of positive example\n",
        "        # 每行第一列是该样本的正样本相似度，后面是其他所有样本（负样本）的相似度\n",
        "        comb_sim = torch.cat([cos_sim[pos_mask][:,None],  # First position positive example\n",
        "                              cos_sim.masked_fill(pos_mask, -9e15)],\n",
        "                             dim=-1)\n",
        "        # 计算正样本的排名:每行降序排序，获取索引。找到值0（第一列）在排序中的位置\n",
        "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
        "        # Logging ranking metrics\n",
        "        self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
        "        self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
        "        self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
        "\n",
        "        return nll\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.info_nce_loss(batch, mode='train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.info_nce_loss(batch, mode='val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlji_oHg6xvn"
      },
      "source": [
        "### Training\n",
        "\n",
        "Now that we have implemented SimCLR and the data loading pipeline, we are ready to train the model. We will use the same training function setup as usual. For saving the best model checkpoint, we track the metric `val_acc_top5`, which describes how often the correct image patch is within the top-5 most similar examples in the batch. This is usually less noisy than the top-1 metric, making it a better metric to choose the best model from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExzYZLBT6xvn"
      },
      "outputs": [],
      "source": [
        "def train_simclr(batch_size, max_epochs=500, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, 'SimCLR'),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc_top5'),\n",
        "                                    LearningRateMonitor('epoch')])\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'SimCLR.ckpt')\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(f'Found pretrained model at {pretrained_filename}, loading...')\n",
        "        model = SimCLR.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
        "    else:\n",
        "        ds = SimCLRUSDataset(root_dir='./data', size=512, data_transforms = contrast_transforms,n_views = 2)\n",
        "        train_loader = data.DataLoader(ds, batch_size=batch_size, shuffle=True,\n",
        "                                       drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "        val_loader = data.DataLoader(train_data_contrast, batch_size=batch_size, shuffle=False,\n",
        "                                     drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "        pl.seed_everything(42) # To be reproducable\n",
        "        model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDKTt4056xvn"
      },
      "source": [
        "A common observation in contrastive learning is that the larger the batch size, the better the models perform. A larger batch size allows us to compare each image to more negative examples, leading to overall smoother loss gradients. However, in our case, we experienced that a batch size of 256 was sufficient to get good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r18aIUM96xvo",
        "outputId": "bf0ec09a-e5de-46ae-e9eb-74b577abfac8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model at ../saved_models/tutorial17/SimCLR.ckpt, loading...\n"
          ]
        }
      ],
      "source": [
        "simclr_model = train_simclr(batch_size=256,\n",
        "                            hidden_dim=128,\n",
        "                            lr=5e-4,\n",
        "                            temperature=0.07,\n",
        "                            weight_decay=1e-4,\n",
        "                            max_epochs=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvVNjH2f6xvo"
      },
      "source": [
        "To get an intuition of how training with contrastive learning behaves, we can take a look at the TensorBoard below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2sx0Kqj6xvo"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir ../saved_models/tutorial17/tensorboards/SimCLR/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBpfkWdh6xvo"
      },
      "source": [
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/tensorboard_simclr.png?raw=1\" width=\"1200px\"></center>\n",
        "\n",
        "One thing to note is that contrastive learning benefits a lot from long training. The shown plot above is from a training that took approx. 1 day on a NVIDIA TitanRTX. Training the model for even longer might reduce its loss further, but we did not experience any gains from it for the downstream task on image classification. In general, contrastive learning can also benefit from using larger models, if sufficient unlabeled data is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcAv-dtF6xvo"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "After we have trained our model via contrastive learning, we can deploy it on downstream tasks and see how well it performs with little data. A common setup, which also verifies whether the model has learned generalized representations, is to perform Logistic Regression on the features. In other words, we learn a single, linear layer that maps the representations to a class prediction. Since the base network $f(\\cdot)$ is not changed during the training process, the model can only perform well if the representations of $h$ describe all features that might be necessary for the task. Further, we do not have to worry too much about overfitting since we have very few parameters that are trained. Hence, we might expect that the model can perform well even with very little data.\n",
        "\n",
        "First, let's implement a simple Logistic Regression setup for which we assume that the images already have been encoded in their feature vectors. If very little data is available, it might be beneficial to dynamically encode the images during training so that we can also apply data augmentations. However, the way we implement it here is much more efficient and can be trained within a few seconds. Further, using data augmentations did not show any significant gain in this simple setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPZnTDTW6xvo"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=100):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        # Mapping from representation h to classes\n",
        "        self.model = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(),\n",
        "                                lr=self.hparams.lr,\n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                      milestones=[int(self.hparams.max_epochs*0.6),\n",
        "                                                                  int(self.hparams.max_epochs*0.8)],\n",
        "                                                      gamma=0.1)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def _calculate_loss(self, batch, mode='train'):\n",
        "        feats, labels = batch\n",
        "        preds = self.model(feats)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        self.log(mode + '_loss', loss)\n",
        "        self.log(mode + '_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._calculate_loss(batch, mode='train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4XqaOiy6xvo"
      },
      "source": [
        "The data we use is the training and test set of STL10. The training contains 500 images per class, while the test set has 800 images per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mge-p_wA6xvp",
        "outputId": "e854dc23-f92c-493d-def3-b371ac8798cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Number of training examples: 5000\n",
            "Number of test examples: 8000\n"
          ]
        }
      ],
      "source": [
        "img_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_img_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                       transform=img_transforms)\n",
        "test_img_data = STL10(root=DATASET_PATH, split='test', download=True,\n",
        "                      transform=img_transforms)\n",
        "\n",
        "print(\"Number of training examples:\", len(train_img_data))\n",
        "print(\"Number of test examples:\", len(test_img_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxZUmGK6xvp"
      },
      "source": [
        "Next, we implement a small function to encode all images in our datasets. The output representations are then used as inputs to the Logistic Regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JEfyaPQ6xvp"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def prepare_data_features(model, dataset):\n",
        "    # Prepare model\n",
        "    network = deepcopy(model.convnet)\n",
        "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
        "    network.eval()\n",
        "    network.to(device)\n",
        "\n",
        "    # Encode all images\n",
        "    data_loader = data.DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
        "    feats, labels = [], []\n",
        "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
        "        batch_imgs = batch_imgs.to(device)\n",
        "        batch_feats = network(batch_imgs)\n",
        "        feats.append(batch_feats.detach().cpu())\n",
        "        labels.append(batch_labels)\n",
        "\n",
        "    feats = torch.cat(feats, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    # Sort images by labels\n",
        "    labels, idxs = labels.sort()\n",
        "    feats = feats[idxs]\n",
        "\n",
        "    return data.TensorDataset(feats, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBqNTpYi6xvp"
      },
      "source": [
        "Let's apply the function to both training and test set below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "51bfd549d76e4f9a82fcdf350de1f796",
            "158ab425a0aa4179ac7c3b449c371aa9"
          ]
        },
        "id": "iM_6vAad6xvp",
        "outputId": "bdb68989-94c2-4727-904f-d8941f2df18d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51bfd549d76e4f9a82fcdf350de1f796",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/79 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "158ab425a0aa4179ac7c3b449c371aa9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_feats_simclr = prepare_data_features(simclr_model, train_img_data)\n",
        "test_feats_simclr = prepare_data_features(simclr_model, test_img_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0osJxrwG6xvp"
      },
      "source": [
        "Finally, we can write a training function as usual. We evaluate the model on the test set every 10 epochs to allow early stopping, but the low frequency of the validation ensures that we do not overfit too much on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG1KmQrk6xvp"
      },
      "outputs": [],
      "source": [
        "def train_logreg(batch_size, train_feats_data, test_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
        "                                    LearningRateMonitor(\"epoch\")],\n",
        "                         enable_progress_bar=False,\n",
        "                         check_val_every_n_epoch=10)\n",
        "    trainer.logger._default_hp_metric = None\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = data.DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
        "                                   drop_last=False, pin_memory=True, num_workers=0)\n",
        "    test_loader = data.DataLoader(test_feats_data, batch_size=batch_size, shuffle=False,\n",
        "                                  drop_last=False, pin_memory=True, num_workers=0)\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
        "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42)  # To be reproducable\n",
        "        model = LogisticRegression(**kwargs)\n",
        "        trainer.fit(model, train_loader, test_loader)\n",
        "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on train and validation set\n",
        "    train_result = trainer.test(model, train_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": test_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVzivpdG6xvp"
      },
      "source": [
        "Despite the training dataset of STL10 already only having 500 labeled images per class, we will perform experiments with even smaller datasets. Specifically, we train a Logistic Regression model for datasets with only 10, 20, 50, 100, 200, and all 500 examples per class. This gives us an intuition on how well the representations learned by contrastive learning can be transfered to a image recognition task like this classification. First, let's define a function to create the intended sub-datasets from the full training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_G5aDQ6xvq"
      },
      "outputs": [],
      "source": [
        "def get_smaller_dataset(original_dataset, num_imgs_per_label):\n",
        "    new_dataset = data.TensorDataset(\n",
        "        *[t.unflatten(0, (10, -1))[:,:num_imgs_per_label].flatten(0, 1) for t in original_dataset.tensors]\n",
        "    )\n",
        "    return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-fkc_GB6xvq"
      },
      "source": [
        "Next, let's run all models. Despite us training 6 models, this cell could be run within a minute or two without the pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1HjjMGp6xvt",
        "outputId": "d24cc83f-0681-4cdd-ebe0-31675146c49a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model at ../saved_models/tutorial17/LogisticRegression_10.ckpt, loading...\n",
            "Found pretrained model at ../saved_models/tutorial17/LogisticRegression_20.ckpt, loading...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model at ../saved_models/tutorial17/LogisticRegression_50.ckpt, loading...\n",
            "Found pretrained model at ../saved_models/tutorial17/LogisticRegression_100.ckpt, loading...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model at ../saved_models/tutorial17/LogisticRegression_200.ckpt, loading...\n",
            "Found pretrained model at ../saved_models/tutorial17/LogisticRegression_500.ckpt, loading...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "for num_imgs_per_label in [10, 20, 50, 100, 200, 500]:\n",
        "    sub_train_set = get_smaller_dataset(train_feats_simclr, num_imgs_per_label)\n",
        "    _, small_set_results = train_logreg(batch_size=64,\n",
        "                                        train_feats_data=sub_train_set,\n",
        "                                        test_feats_data=test_feats_simclr,\n",
        "                                        model_suffix=num_imgs_per_label,\n",
        "                                        feature_dim=train_feats_simclr.tensors[0].shape[1],\n",
        "                                        num_classes=10,\n",
        "                                        lr=1e-3,\n",
        "                                        weight_decay=1e-3)\n",
        "    results[num_imgs_per_label] = small_set_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiyiTMMA6xvt"
      },
      "source": [
        "Finally, let's plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ffLiYYH6xvu",
        "outputId": "62b0c917-0660-4ca0-a155-b24b7b9137a5"
      },
      "outputs": [
        {
          "data": {
            "application/pdf": "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\n",
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
              "<svg height=\"282.682031pt\" version=\"1.1\" viewBox=\"0 0 401.339063 282.682031\" width=\"401.339063pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
              " <metadata>\n",
              "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
              "   <cc:Work>\n",
              "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
              "    <dc:date>2021-08-30T10:54:05.267743</dc:date>\n",
              "    <dc:format>image/svg+xml</dc:format>\n",
              "    <dc:creator>\n",
              "     <cc:Agent>\n",
              "      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n",
              "     </cc:Agent>\n",
              "    </dc:creator>\n",
              "   </cc:Work>\n",
              "  </rdf:RDF>\n",
              " </metadata>\n",
              " <defs>\n",
              "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
              " </defs>\n",
              " <g id=\"figure_1\">\n",
              "  <g id=\"patch_1\">\n",
              "   <path d=\"M 0 282.682031 \n",
              "L 401.339063 282.682031 \n",
              "L 401.339063 0 \n",
              "L 0 0 \n",
              "z\n",
              "\" style=\"fill:#ffffff;\"/>\n",
              "  </g>\n",
              "  <g id=\"axes_1\">\n",
              "   <g id=\"patch_2\">\n",
              "    <path d=\"M 59.339063 240.660937 \n",
              "L 394.139063 240.660937 \n",
              "L 394.139063 23.220937 \n",
              "L 59.339063 23.220937 \n",
              "z\n",
              "\" style=\"fill:#eaeaf2;\"/>\n",
              "   </g>\n",
              "   <g id=\"matplotlib.axis_1\">\n",
              "    <g id=\"xtick_1\">\n",
              "     <g id=\"line2d_1\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 74.557244 240.660937 \n",
              "L 74.557244 23.220937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_1\">\n",
              "      <!-- 10 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(68.440213 258.034531)scale(0.11 -0.11)\">\n",
              "       <defs>\n",
              "        <path d=\"M 37.25 0 \n",
              "L 28.46875 0 \n",
              "L 28.46875 56 \n",
              "Q 25.296875 52.984375 20.140625 49.953125 \n",
              "Q 14.984375 46.921875 10.890625 45.40625 \n",
              "L 10.890625 53.90625 \n",
              "Q 18.265625 57.375 23.78125 62.296875 \n",
              "Q 29.296875 67.234375 31.59375 71.875 \n",
              "L 37.25 71.875 \n",
              "z\n",
              "\" id=\"ArialMT-49\"/>\n",
              "        <path d=\"M 4.15625 35.296875 \n",
              "Q 4.15625 48 6.765625 55.734375 \n",
              "Q 9.375 63.484375 14.515625 67.671875 \n",
              "Q 19.671875 71.875 27.484375 71.875 \n",
              "Q 33.25 71.875 37.59375 69.546875 \n",
              "Q 41.9375 67.234375 44.765625 62.859375 \n",
              "Q 47.609375 58.5 49.21875 52.21875 \n",
              "Q 50.828125 45.953125 50.828125 35.296875 \n",
              "Q 50.828125 22.703125 48.234375 14.96875 \n",
              "Q 45.65625 7.234375 40.5 3 \n",
              "Q 35.359375 -1.21875 27.484375 -1.21875 \n",
              "Q 17.140625 -1.21875 11.234375 6.203125 \n",
              "Q 4.15625 15.140625 4.15625 35.296875 \n",
              "z\n",
              "M 13.1875 35.296875 \n",
              "Q 13.1875 17.671875 17.3125 11.828125 \n",
              "Q 21.4375 6 27.484375 6 \n",
              "Q 33.546875 6 37.671875 11.859375 \n",
              "Q 41.796875 17.71875 41.796875 35.296875 \n",
              "Q 41.796875 52.984375 37.671875 58.78125 \n",
              "Q 33.546875 64.59375 27.390625 64.59375 \n",
              "Q 21.34375 64.59375 17.71875 59.46875 \n",
              "Q 13.1875 52.9375 13.1875 35.296875 \n",
              "z\n",
              "\" id=\"ArialMT-48\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#ArialMT-49\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_2\">\n",
              "     <g id=\"line2d_2\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 128.485556 240.660937 \n",
              "L 128.485556 23.220937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_2\">\n",
              "      <!-- 20 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(122.368525 258.034531)scale(0.11 -0.11)\">\n",
              "       <defs>\n",
              "        <path d=\"M 50.34375 8.453125 \n",
              "L 50.34375 0 \n",
              "L 3.03125 0 \n",
              "Q 2.9375 3.171875 4.046875 6.109375 \n",
              "Q 5.859375 10.9375 9.828125 15.625 \n",
              "Q 13.8125 20.3125 21.34375 26.46875 \n",
              "Q 33.015625 36.03125 37.109375 41.625 \n",
              "Q 41.21875 47.21875 41.21875 52.203125 \n",
              "Q 41.21875 57.421875 37.46875 61 \n",
              "Q 33.734375 64.59375 27.734375 64.59375 \n",
              "Q 21.390625 64.59375 17.578125 60.78125 \n",
              "Q 13.765625 56.984375 13.71875 50.25 \n",
              "L 4.6875 51.171875 \n",
              "Q 5.609375 61.28125 11.65625 66.578125 \n",
              "Q 17.71875 71.875 27.9375 71.875 \n",
              "Q 38.234375 71.875 44.234375 66.15625 \n",
              "Q 50.25 60.453125 50.25 52 \n",
              "Q 50.25 47.703125 48.484375 43.546875 \n",
              "Q 46.734375 39.40625 42.65625 34.8125 \n",
              "Q 38.578125 30.21875 29.109375 22.21875 \n",
              "Q 21.1875 15.578125 18.9375 13.203125 \n",
              "Q 16.703125 10.84375 15.234375 8.453125 \n",
              "z\n",
              "\" id=\"ArialMT-50\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#ArialMT-50\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_3\">\n",
              "     <g id=\"line2d_3\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 199.774907 240.660937 \n",
              "L 199.774907 23.220937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_3\">\n",
              "      <!-- 50 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(193.657875 258.034531)scale(0.11 -0.11)\">\n",
              "       <defs>\n",
              "        <path d=\"M 4.15625 18.75 \n",
              "L 13.375 19.53125 \n",
              "Q 14.40625 12.796875 18.140625 9.390625 \n",
              "Q 21.875 6 27.15625 6 \n",
              "Q 33.5 6 37.890625 10.78125 \n",
              "Q 42.28125 15.578125 42.28125 23.484375 \n",
              "Q 42.28125 31 38.0625 35.34375 \n",
              "Q 33.84375 39.703125 27 39.703125 \n",
              "Q 22.75 39.703125 19.328125 37.765625 \n",
              "Q 15.921875 35.84375 13.96875 32.765625 \n",
              "L 5.71875 33.84375 \n",
              "L 12.640625 70.609375 \n",
              "L 48.25 70.609375 \n",
              "L 48.25 62.203125 \n",
              "L 19.671875 62.203125 \n",
              "L 15.828125 42.96875 \n",
              "Q 22.265625 47.46875 29.34375 47.46875 \n",
              "Q 38.71875 47.46875 45.15625 40.96875 \n",
              "Q 51.609375 34.46875 51.609375 24.265625 \n",
              "Q 51.609375 14.546875 45.953125 7.46875 \n",
              "Q 39.0625 -1.21875 27.15625 -1.21875 \n",
              "Q 17.390625 -1.21875 11.203125 4.25 \n",
              "Q 5.03125 9.71875 4.15625 18.75 \n",
              "z\n",
              "\" id=\"ArialMT-53\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#ArialMT-53\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_4\">\n",
              "     <g id=\"line2d_4\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 253.703218 240.660937 \n",
              "L 253.703218 23.220937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_4\">\n",
              "      <!-- 100 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(244.527672 258.034531)scale(0.11 -0.11)\">\n",
              "       <use xlink:href=\"#ArialMT-49\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_5\">\n",
              "     <g id=\"line2d_5\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 307.63153 240.660937 \n",
              "L 307.63153 23.220937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_5\">\n",
              "      <!-- 200 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(298.455983 258.034531)scale(0.11 -0.11)\">\n",
              "       <use xlink:href=\"#ArialMT-50\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"xtick_6\">\n",
              "     <g id=\"line2d_6\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 378.920881 240.660937 \n",
              "L 378.920881 23.220937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_6\">\n",
              "      <!-- 500 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(369.745334 258.034531)scale(0.11 -0.11)\">\n",
              "       <use xlink:href=\"#ArialMT-53\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"text_7\">\n",
              "     <!-- Number of images per class -->\n",
              "     <g style=\"fill:#262626;\" transform=\"translate(152.048438 272.956406)scale(0.12 -0.12)\">\n",
              "      <defs>\n",
              "       <path d=\"M 7.625 0 \n",
              "L 7.625 71.578125 \n",
              "L 17.328125 71.578125 \n",
              "L 54.9375 15.375 \n",
              "L 54.9375 71.578125 \n",
              "L 64.015625 71.578125 \n",
              "L 64.015625 0 \n",
              "L 54.296875 0 \n",
              "L 16.703125 56.25 \n",
              "L 16.703125 0 \n",
              "z\n",
              "\" id=\"ArialMT-78\"/>\n",
              "       <path d=\"M 40.578125 0 \n",
              "L 40.578125 7.625 \n",
              "Q 34.515625 -1.171875 24.125 -1.171875 \n",
              "Q 19.53125 -1.171875 15.546875 0.578125 \n",
              "Q 11.578125 2.34375 9.640625 5 \n",
              "Q 7.71875 7.671875 6.9375 11.53125 \n",
              "Q 6.390625 14.109375 6.390625 19.734375 \n",
              "L 6.390625 51.859375 \n",
              "L 15.1875 51.859375 \n",
              "L 15.1875 23.09375 \n",
              "Q 15.1875 16.21875 15.71875 13.8125 \n",
              "Q 16.546875 10.359375 19.234375 8.375 \n",
              "Q 21.921875 6.390625 25.875 6.390625 \n",
              "Q 29.828125 6.390625 33.296875 8.421875 \n",
              "Q 36.765625 10.453125 38.203125 13.9375 \n",
              "Q 39.65625 17.4375 39.65625 24.078125 \n",
              "L 39.65625 51.859375 \n",
              "L 48.4375 51.859375 \n",
              "L 48.4375 0 \n",
              "z\n",
              "\" id=\"ArialMT-117\"/>\n",
              "       <path d=\"M 6.59375 0 \n",
              "L 6.59375 51.859375 \n",
              "L 14.453125 51.859375 \n",
              "L 14.453125 44.578125 \n",
              "Q 16.890625 48.390625 20.9375 50.703125 \n",
              "Q 25 53.03125 30.171875 53.03125 \n",
              "Q 35.9375 53.03125 39.625 50.640625 \n",
              "Q 43.3125 48.25 44.828125 43.953125 \n",
              "Q 50.984375 53.03125 60.84375 53.03125 \n",
              "Q 68.5625 53.03125 72.703125 48.75 \n",
              "Q 76.859375 44.484375 76.859375 35.59375 \n",
              "L 76.859375 0 \n",
              "L 68.109375 0 \n",
              "L 68.109375 32.671875 \n",
              "Q 68.109375 37.9375 67.25 40.25 \n",
              "Q 66.40625 42.578125 64.15625 43.984375 \n",
              "Q 61.921875 45.40625 58.890625 45.40625 \n",
              "Q 53.421875 45.40625 49.796875 41.765625 \n",
              "Q 46.1875 38.140625 46.1875 30.125 \n",
              "L 46.1875 0 \n",
              "L 37.40625 0 \n",
              "L 37.40625 33.6875 \n",
              "Q 37.40625 39.546875 35.25 42.46875 \n",
              "Q 33.109375 45.40625 28.21875 45.40625 \n",
              "Q 24.515625 45.40625 21.359375 43.453125 \n",
              "Q 18.21875 41.5 16.796875 37.734375 \n",
              "Q 15.375 33.984375 15.375 26.90625 \n",
              "L 15.375 0 \n",
              "z\n",
              "\" id=\"ArialMT-109\"/>\n",
              "       <path d=\"M 14.703125 0 \n",
              "L 6.546875 0 \n",
              "L 6.546875 71.578125 \n",
              "L 15.328125 71.578125 \n",
              "L 15.328125 46.046875 \n",
              "Q 20.90625 53.03125 29.546875 53.03125 \n",
              "Q 34.328125 53.03125 38.59375 51.09375 \n",
              "Q 42.875 49.171875 45.625 45.671875 \n",
              "Q 48.390625 42.1875 49.953125 37.25 \n",
              "Q 51.515625 32.328125 51.515625 26.703125 \n",
              "Q 51.515625 13.375 44.921875 6.09375 \n",
              "Q 38.328125 -1.171875 29.109375 -1.171875 \n",
              "Q 19.921875 -1.171875 14.703125 6.5 \n",
              "z\n",
              "M 14.59375 26.3125 \n",
              "Q 14.59375 17 17.140625 12.84375 \n",
              "Q 21.296875 6.0625 28.375 6.0625 \n",
              "Q 34.125 6.0625 38.328125 11.0625 \n",
              "Q 42.53125 16.0625 42.53125 25.984375 \n",
              "Q 42.53125 36.140625 38.5 40.96875 \n",
              "Q 34.46875 45.796875 28.765625 45.796875 \n",
              "Q 23 45.796875 18.796875 40.796875 \n",
              "Q 14.59375 35.796875 14.59375 26.3125 \n",
              "z\n",
              "\" id=\"ArialMT-98\"/>\n",
              "       <path d=\"M 42.09375 16.703125 \n",
              "L 51.171875 15.578125 \n",
              "Q 49.03125 7.625 43.21875 3.21875 \n",
              "Q 37.40625 -1.171875 28.375 -1.171875 \n",
              "Q 17 -1.171875 10.328125 5.828125 \n",
              "Q 3.65625 12.84375 3.65625 25.484375 \n",
              "Q 3.65625 38.578125 10.390625 45.796875 \n",
              "Q 17.140625 53.03125 27.875 53.03125 \n",
              "Q 38.28125 53.03125 44.875 45.953125 \n",
              "Q 51.46875 38.875 51.46875 26.03125 \n",
              "Q 51.46875 25.25 51.421875 23.6875 \n",
              "L 12.75 23.6875 \n",
              "Q 13.234375 15.140625 17.578125 10.59375 \n",
              "Q 21.921875 6.0625 28.421875 6.0625 \n",
              "Q 33.25 6.0625 36.671875 8.59375 \n",
              "Q 40.09375 11.140625 42.09375 16.703125 \n",
              "z\n",
              "M 13.234375 30.90625 \n",
              "L 42.1875 30.90625 \n",
              "Q 41.609375 37.453125 38.875 40.71875 \n",
              "Q 34.671875 45.796875 27.984375 45.796875 \n",
              "Q 21.921875 45.796875 17.796875 41.75 \n",
              "Q 13.671875 37.703125 13.234375 30.90625 \n",
              "z\n",
              "\" id=\"ArialMT-101\"/>\n",
              "       <path d=\"M 6.5 0 \n",
              "L 6.5 51.859375 \n",
              "L 14.40625 51.859375 \n",
              "L 14.40625 44 \n",
              "Q 17.4375 49.515625 20 51.265625 \n",
              "Q 22.5625 53.03125 25.640625 53.03125 \n",
              "Q 30.078125 53.03125 34.671875 50.203125 \n",
              "L 31.640625 42.046875 \n",
              "Q 28.421875 43.953125 25.203125 43.953125 \n",
              "Q 22.3125 43.953125 20.015625 42.21875 \n",
              "Q 17.71875 40.484375 16.75 37.40625 \n",
              "Q 15.28125 32.71875 15.28125 27.15625 \n",
              "L 15.28125 0 \n",
              "z\n",
              "\" id=\"ArialMT-114\"/>\n",
              "       <path id=\"ArialMT-32\"/>\n",
              "       <path d=\"M 3.328125 25.921875 \n",
              "Q 3.328125 40.328125 11.328125 47.265625 \n",
              "Q 18.015625 53.03125 27.640625 53.03125 \n",
              "Q 38.328125 53.03125 45.109375 46.015625 \n",
              "Q 51.90625 39.015625 51.90625 26.65625 \n",
              "Q 51.90625 16.65625 48.90625 10.90625 \n",
              "Q 45.90625 5.171875 40.15625 2 \n",
              "Q 34.421875 -1.171875 27.640625 -1.171875 \n",
              "Q 16.75 -1.171875 10.03125 5.8125 \n",
              "Q 3.328125 12.796875 3.328125 25.921875 \n",
              "z\n",
              "M 12.359375 25.921875 \n",
              "Q 12.359375 15.96875 16.703125 11.015625 \n",
              "Q 21.046875 6.0625 27.640625 6.0625 \n",
              "Q 34.1875 6.0625 38.53125 11.03125 \n",
              "Q 42.875 16.015625 42.875 26.21875 \n",
              "Q 42.875 35.84375 38.5 40.796875 \n",
              "Q 34.125 45.75 27.640625 45.75 \n",
              "Q 21.046875 45.75 16.703125 40.8125 \n",
              "Q 12.359375 35.890625 12.359375 25.921875 \n",
              "z\n",
              "\" id=\"ArialMT-111\"/>\n",
              "       <path d=\"M 8.6875 0 \n",
              "L 8.6875 45.015625 \n",
              "L 0.921875 45.015625 \n",
              "L 0.921875 51.859375 \n",
              "L 8.6875 51.859375 \n",
              "L 8.6875 57.375 \n",
              "Q 8.6875 62.59375 9.625 65.140625 \n",
              "Q 10.890625 68.5625 14.078125 70.671875 \n",
              "Q 17.28125 72.796875 23.046875 72.796875 \n",
              "Q 26.765625 72.796875 31.25 71.921875 \n",
              "L 29.9375 64.265625 \n",
              "Q 27.203125 64.75 24.75 64.75 \n",
              "Q 20.75 64.75 19.09375 63.03125 \n",
              "Q 17.4375 61.328125 17.4375 56.640625 \n",
              "L 17.4375 51.859375 \n",
              "L 27.546875 51.859375 \n",
              "L 27.546875 45.015625 \n",
              "L 17.4375 45.015625 \n",
              "L 17.4375 0 \n",
              "z\n",
              "\" id=\"ArialMT-102\"/>\n",
              "       <path d=\"M 6.640625 61.46875 \n",
              "L 6.640625 71.578125 \n",
              "L 15.4375 71.578125 \n",
              "L 15.4375 61.46875 \n",
              "z\n",
              "M 6.640625 0 \n",
              "L 6.640625 51.859375 \n",
              "L 15.4375 51.859375 \n",
              "L 15.4375 0 \n",
              "z\n",
              "\" id=\"ArialMT-105\"/>\n",
              "       <path d=\"M 40.4375 6.390625 \n",
              "Q 35.546875 2.25 31.03125 0.53125 \n",
              "Q 26.515625 -1.171875 21.34375 -1.171875 \n",
              "Q 12.796875 -1.171875 8.203125 3 \n",
              "Q 3.609375 7.171875 3.609375 13.671875 \n",
              "Q 3.609375 17.484375 5.34375 20.625 \n",
              "Q 7.078125 23.78125 9.890625 25.6875 \n",
              "Q 12.703125 27.59375 16.21875 28.5625 \n",
              "Q 18.796875 29.25 24.03125 29.890625 \n",
              "Q 34.671875 31.15625 39.703125 32.90625 \n",
              "Q 39.75 34.71875 39.75 35.203125 \n",
              "Q 39.75 40.578125 37.25 42.78125 \n",
              "Q 33.890625 45.75 27.25 45.75 \n",
              "Q 21.046875 45.75 18.09375 43.578125 \n",
              "Q 15.140625 41.40625 13.71875 35.890625 \n",
              "L 5.125 37.0625 \n",
              "Q 6.296875 42.578125 8.984375 45.96875 \n",
              "Q 11.671875 49.359375 16.75 51.1875 \n",
              "Q 21.828125 53.03125 28.515625 53.03125 \n",
              "Q 35.15625 53.03125 39.296875 51.46875 \n",
              "Q 43.453125 49.90625 45.40625 47.53125 \n",
              "Q 47.359375 45.171875 48.140625 41.546875 \n",
              "Q 48.578125 39.3125 48.578125 33.453125 \n",
              "L 48.578125 21.734375 \n",
              "Q 48.578125 9.46875 49.140625 6.21875 \n",
              "Q 49.703125 2.984375 51.375 0 \n",
              "L 42.1875 0 \n",
              "Q 40.828125 2.734375 40.4375 6.390625 \n",
              "z\n",
              "M 39.703125 26.03125 \n",
              "Q 34.90625 24.078125 25.34375 22.703125 \n",
              "Q 19.921875 21.921875 17.671875 20.9375 \n",
              "Q 15.4375 19.96875 14.203125 18.09375 \n",
              "Q 12.984375 16.21875 12.984375 13.921875 \n",
              "Q 12.984375 10.40625 15.640625 8.0625 \n",
              "Q 18.3125 5.71875 23.4375 5.71875 \n",
              "Q 28.515625 5.71875 32.46875 7.9375 \n",
              "Q 36.421875 10.15625 38.28125 14.015625 \n",
              "Q 39.703125 17 39.703125 22.796875 \n",
              "z\n",
              "\" id=\"ArialMT-97\"/>\n",
              "       <path d=\"M 4.984375 -4.296875 \n",
              "L 13.53125 -5.5625 \n",
              "Q 14.0625 -9.515625 16.5 -11.328125 \n",
              "Q 19.78125 -13.765625 25.4375 -13.765625 \n",
              "Q 31.546875 -13.765625 34.859375 -11.328125 \n",
              "Q 38.1875 -8.890625 39.359375 -4.5 \n",
              "Q 40.046875 -1.8125 39.984375 6.78125 \n",
              "Q 34.234375 0 25.640625 0 \n",
              "Q 14.9375 0 9.078125 7.71875 \n",
              "Q 3.21875 15.4375 3.21875 26.21875 \n",
              "Q 3.21875 33.640625 5.90625 39.90625 \n",
              "Q 8.59375 46.1875 13.6875 49.609375 \n",
              "Q 18.796875 53.03125 25.6875 53.03125 \n",
              "Q 34.859375 53.03125 40.828125 45.609375 \n",
              "L 40.828125 51.859375 \n",
              "L 48.921875 51.859375 \n",
              "L 48.921875 7.03125 \n",
              "Q 48.921875 -5.078125 46.453125 -10.125 \n",
              "Q 44 -15.1875 38.640625 -18.109375 \n",
              "Q 33.296875 -21.046875 25.484375 -21.046875 \n",
              "Q 16.21875 -21.046875 10.5 -16.875 \n",
              "Q 4.78125 -12.703125 4.984375 -4.296875 \n",
              "z\n",
              "M 12.25 26.859375 \n",
              "Q 12.25 16.65625 16.296875 11.96875 \n",
              "Q 20.359375 7.28125 26.46875 7.28125 \n",
              "Q 32.515625 7.28125 36.609375 11.9375 \n",
              "Q 40.71875 16.609375 40.71875 26.5625 \n",
              "Q 40.71875 36.078125 36.5 40.90625 \n",
              "Q 32.28125 45.75 26.3125 45.75 \n",
              "Q 20.453125 45.75 16.34375 40.984375 \n",
              "Q 12.25 36.234375 12.25 26.859375 \n",
              "z\n",
              "\" id=\"ArialMT-103\"/>\n",
              "       <path d=\"M 3.078125 15.484375 \n",
              "L 11.765625 16.84375 \n",
              "Q 12.5 11.625 15.84375 8.84375 \n",
              "Q 19.1875 6.0625 25.203125 6.0625 \n",
              "Q 31.25 6.0625 34.171875 8.515625 \n",
              "Q 37.109375 10.984375 37.109375 14.3125 \n",
              "Q 37.109375 17.28125 34.515625 19 \n",
              "Q 32.71875 20.171875 25.53125 21.96875 \n",
              "Q 15.875 24.421875 12.140625 26.203125 \n",
              "Q 8.40625 27.984375 6.46875 31.125 \n",
              "Q 4.546875 34.28125 4.546875 38.09375 \n",
              "Q 4.546875 41.546875 6.125 44.5 \n",
              "Q 7.71875 47.46875 10.453125 49.421875 \n",
              "Q 12.5 50.921875 16.03125 51.96875 \n",
              "Q 19.578125 53.03125 23.640625 53.03125 \n",
              "Q 29.734375 53.03125 34.34375 51.265625 \n",
              "Q 38.96875 49.515625 41.15625 46.5 \n",
              "Q 43.359375 43.5 44.1875 38.484375 \n",
              "L 35.59375 37.3125 \n",
              "Q 35.015625 41.3125 32.203125 43.546875 \n",
              "Q 29.390625 45.796875 24.265625 45.796875 \n",
              "Q 18.21875 45.796875 15.625 43.796875 \n",
              "Q 13.03125 41.796875 13.03125 39.109375 \n",
              "Q 13.03125 37.40625 14.109375 36.03125 \n",
              "Q 15.1875 34.625 17.484375 33.6875 \n",
              "Q 18.796875 33.203125 25.25 31.453125 \n",
              "Q 34.578125 28.953125 38.25 27.359375 \n",
              "Q 41.9375 25.78125 44.03125 22.75 \n",
              "Q 46.140625 19.734375 46.140625 15.234375 \n",
              "Q 46.140625 10.84375 43.578125 6.953125 \n",
              "Q 41.015625 3.078125 36.171875 0.953125 \n",
              "Q 31.34375 -1.171875 25.25 -1.171875 \n",
              "Q 15.140625 -1.171875 9.84375 3.03125 \n",
              "Q 4.546875 7.234375 3.078125 15.484375 \n",
              "z\n",
              "\" id=\"ArialMT-115\"/>\n",
              "       <path d=\"M 6.59375 -19.875 \n",
              "L 6.59375 51.859375 \n",
              "L 14.59375 51.859375 \n",
              "L 14.59375 45.125 \n",
              "Q 17.4375 49.078125 21 51.046875 \n",
              "Q 24.5625 53.03125 29.640625 53.03125 \n",
              "Q 36.28125 53.03125 41.359375 49.609375 \n",
              "Q 46.4375 46.1875 49.015625 39.953125 \n",
              "Q 51.609375 33.734375 51.609375 26.3125 \n",
              "Q 51.609375 18.359375 48.75 11.984375 \n",
              "Q 45.90625 5.609375 40.453125 2.21875 \n",
              "Q 35.015625 -1.171875 29 -1.171875 \n",
              "Q 24.609375 -1.171875 21.109375 0.6875 \n",
              "Q 17.625 2.546875 15.375 5.375 \n",
              "L 15.375 -19.875 \n",
              "z\n",
              "M 14.546875 25.640625 \n",
              "Q 14.546875 15.625 18.59375 10.84375 \n",
              "Q 22.65625 6.0625 28.421875 6.0625 \n",
              "Q 34.28125 6.0625 38.453125 11.015625 \n",
              "Q 42.625 15.96875 42.625 26.375 \n",
              "Q 42.625 36.28125 38.546875 41.203125 \n",
              "Q 34.46875 46.140625 28.8125 46.140625 \n",
              "Q 23.1875 46.140625 18.859375 40.890625 \n",
              "Q 14.546875 35.640625 14.546875 25.640625 \n",
              "z\n",
              "\" id=\"ArialMT-112\"/>\n",
              "       <path d=\"M 40.4375 19 \n",
              "L 49.078125 17.875 \n",
              "Q 47.65625 8.9375 41.8125 3.875 \n",
              "Q 35.984375 -1.171875 27.484375 -1.171875 \n",
              "Q 16.84375 -1.171875 10.375 5.78125 \n",
              "Q 3.90625 12.75 3.90625 25.734375 \n",
              "Q 3.90625 34.125 6.6875 40.421875 \n",
              "Q 9.46875 46.734375 15.15625 49.875 \n",
              "Q 20.84375 53.03125 27.546875 53.03125 \n",
              "Q 35.984375 53.03125 41.359375 48.75 \n",
              "Q 46.734375 44.484375 48.25 36.625 \n",
              "L 39.703125 35.296875 \n",
              "Q 38.484375 40.53125 35.375 43.15625 \n",
              "Q 32.28125 45.796875 27.875 45.796875 \n",
              "Q 21.234375 45.796875 17.078125 41.03125 \n",
              "Q 12.9375 36.28125 12.9375 25.984375 \n",
              "Q 12.9375 15.53125 16.9375 10.796875 \n",
              "Q 20.953125 6.0625 27.390625 6.0625 \n",
              "Q 32.5625 6.0625 36.03125 9.234375 \n",
              "Q 39.5 12.40625 40.4375 19 \n",
              "z\n",
              "\" id=\"ArialMT-99\"/>\n",
              "       <path d=\"M 6.390625 0 \n",
              "L 6.390625 71.578125 \n",
              "L 15.1875 71.578125 \n",
              "L 15.1875 0 \n",
              "z\n",
              "\" id=\"ArialMT-108\"/>\n",
              "      </defs>\n",
              "      <use xlink:href=\"#ArialMT-78\"/>\n",
              "      <use x=\"72.216797\" xlink:href=\"#ArialMT-117\"/>\n",
              "      <use x=\"127.832031\" xlink:href=\"#ArialMT-109\"/>\n",
              "      <use x=\"211.132812\" xlink:href=\"#ArialMT-98\"/>\n",
              "      <use x=\"266.748047\" xlink:href=\"#ArialMT-101\"/>\n",
              "      <use x=\"322.363281\" xlink:href=\"#ArialMT-114\"/>\n",
              "      <use x=\"355.664062\" xlink:href=\"#ArialMT-32\"/>\n",
              "      <use x=\"383.447266\" xlink:href=\"#ArialMT-111\"/>\n",
              "      <use x=\"439.0625\" xlink:href=\"#ArialMT-102\"/>\n",
              "      <use x=\"466.845703\" xlink:href=\"#ArialMT-32\"/>\n",
              "      <use x=\"494.628906\" xlink:href=\"#ArialMT-105\"/>\n",
              "      <use x=\"516.845703\" xlink:href=\"#ArialMT-109\"/>\n",
              "      <use x=\"600.146484\" xlink:href=\"#ArialMT-97\"/>\n",
              "      <use x=\"655.761719\" xlink:href=\"#ArialMT-103\"/>\n",
              "      <use x=\"711.376953\" xlink:href=\"#ArialMT-101\"/>\n",
              "      <use x=\"766.992188\" xlink:href=\"#ArialMT-115\"/>\n",
              "      <use x=\"816.992188\" xlink:href=\"#ArialMT-32\"/>\n",
              "      <use x=\"844.775391\" xlink:href=\"#ArialMT-112\"/>\n",
              "      <use x=\"900.390625\" xlink:href=\"#ArialMT-101\"/>\n",
              "      <use x=\"956.005859\" xlink:href=\"#ArialMT-114\"/>\n",
              "      <use x=\"989.306641\" xlink:href=\"#ArialMT-32\"/>\n",
              "      <use x=\"1017.089844\" xlink:href=\"#ArialMT-99\"/>\n",
              "      <use x=\"1067.089844\" xlink:href=\"#ArialMT-108\"/>\n",
              "      <use x=\"1089.306641\" xlink:href=\"#ArialMT-97\"/>\n",
              "      <use x=\"1144.921875\" xlink:href=\"#ArialMT-115\"/>\n",
              "      <use x=\"1194.921875\" xlink:href=\"#ArialMT-115\"/>\n",
              "     </g>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"matplotlib.axis_2\">\n",
              "    <g id=\"ytick_1\">\n",
              "     <g id=\"line2d_7\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 59.339063 233.842996 \n",
              "L 394.139063 233.842996 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_8\">\n",
              "      <!-- 0.625 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(22.315 237.779793)scale(0.11 -0.11)\">\n",
              "       <defs>\n",
              "        <path d=\"M 9.078125 0 \n",
              "L 9.078125 10.015625 \n",
              "L 19.09375 10.015625 \n",
              "L 19.09375 0 \n",
              "z\n",
              "\" id=\"ArialMT-46\"/>\n",
              "        <path d=\"M 49.75 54.046875 \n",
              "L 41.015625 53.375 \n",
              "Q 39.84375 58.546875 37.703125 60.890625 \n",
              "Q 34.125 64.65625 28.90625 64.65625 \n",
              "Q 24.703125 64.65625 21.53125 62.3125 \n",
              "Q 17.390625 59.28125 14.984375 53.46875 \n",
              "Q 12.59375 47.65625 12.5 36.921875 \n",
              "Q 15.671875 41.75 20.265625 44.09375 \n",
              "Q 24.859375 46.4375 29.890625 46.4375 \n",
              "Q 38.671875 46.4375 44.84375 39.96875 \n",
              "Q 51.03125 33.5 51.03125 23.25 \n",
              "Q 51.03125 16.5 48.125 10.71875 \n",
              "Q 45.21875 4.9375 40.140625 1.859375 \n",
              "Q 35.0625 -1.21875 28.609375 -1.21875 \n",
              "Q 17.625 -1.21875 10.6875 6.859375 \n",
              "Q 3.765625 14.9375 3.765625 33.5 \n",
              "Q 3.765625 54.25 11.421875 63.671875 \n",
              "Q 18.109375 71.875 29.4375 71.875 \n",
              "Q 37.890625 71.875 43.28125 67.140625 \n",
              "Q 48.6875 62.40625 49.75 54.046875 \n",
              "z\n",
              "M 13.875 23.1875 \n",
              "Q 13.875 18.65625 15.796875 14.5 \n",
              "Q 17.71875 10.359375 21.1875 8.171875 \n",
              "Q 24.65625 6 28.46875 6 \n",
              "Q 34.03125 6 38.03125 10.484375 \n",
              "Q 42.046875 14.984375 42.046875 22.703125 \n",
              "Q 42.046875 30.125 38.078125 34.390625 \n",
              "Q 34.125 38.671875 28.125 38.671875 \n",
              "Q 22.171875 38.671875 18.015625 34.390625 \n",
              "Q 13.875 30.125 13.875 23.1875 \n",
              "z\n",
              "\" id=\"ArialMT-54\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
              "       <use x=\"83.398438\" xlink:href=\"#ArialMT-54\"/>\n",
              "       <use x=\"139.013672\" xlink:href=\"#ArialMT-50\"/>\n",
              "       <use x=\"194.628906\" xlink:href=\"#ArialMT-53\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_2\">\n",
              "     <g id=\"line2d_8\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 59.339063 207.184508 \n",
              "L 394.139063 207.184508 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_9\">\n",
              "      <!-- 0.650 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(22.315 211.121304)scale(0.11 -0.11)\">\n",
              "       <use xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
              "       <use x=\"83.398438\" xlink:href=\"#ArialMT-54\"/>\n",
              "       <use x=\"139.013672\" xlink:href=\"#ArialMT-53\"/>\n",
              "       <use x=\"194.628906\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_3\">\n",
              "     <g id=\"line2d_9\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 59.339063 180.526019 \n",
              "L 394.139063 180.526019 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_10\">\n",
              "      <!-- 0.675 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(22.315 184.462816)scale(0.11 -0.11)\">\n",
              "       <defs>\n",
              "        <path d=\"M 4.734375 62.203125 \n",
              "L 4.734375 70.65625 \n",
              "L 51.078125 70.65625 \n",
              "L 51.078125 63.8125 \n",
              "Q 44.234375 56.546875 37.515625 44.484375 \n",
              "Q 30.8125 32.421875 27.15625 19.671875 \n",
              "Q 24.515625 10.6875 23.78125 0 \n",
              "L 14.75 0 \n",
              "Q 14.890625 8.453125 18.0625 20.40625 \n",
              "Q 21.234375 32.375 27.171875 43.484375 \n",
              "Q 33.109375 54.59375 39.796875 62.203125 \n",
              "z\n",
              "\" id=\"ArialMT-55\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
              "       <use x=\"83.398438\" xlink:href=\"#ArialMT-54\"/>\n",
              "       <use x=\"139.013672\" xlink:href=\"#ArialMT-55\"/>\n",
              "       <use x=\"194.628906\" xlink:href=\"#ArialMT-53\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_4\">\n",
              "     <g id=\"line2d_10\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 59.339063 153.867531 \n",
              "L 394.139063 153.867531 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_11\">\n",
              "      <!-- 0.700 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(22.315 157.804328)scale(0.11 -0.11)\">\n",
              "       <use xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
              "       <use x=\"83.398438\" xlink:href=\"#ArialMT-55\"/>\n",
              "       <use x=\"139.013672\" xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"194.628906\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_5\">\n",
              "     <g id=\"line2d_11\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 59.339063 127.209043 \n",
              "L 394.139063 127.209043 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_12\">\n",
              "      <!-- 0.725 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(22.315 131.14584)scale(0.11 -0.11)\">\n",
              "       <use xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
              "       <use x=\"83.398438\" xlink:href=\"#ArialMT-55\"/>\n",
              "       <use x=\"139.013672\" xlink:href=\"#ArialMT-50\"/>\n",
              "       <use x=\"194.628906\" xlink:href=\"#ArialMT-53\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_6\">\n",
              "     <g id=\"line2d_12\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 59.339063 100.550555 \n",
              "L 394.139063 100.550555 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_13\">\n",
              "      <!-- 0.750 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(22.315 104.487352)scale(0.11 -0.11)\">\n",
              "       <use xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
              "       <use x=\"83.398438\" xlink:href=\"#ArialMT-55\"/>\n",
              "       <use x=\"139.013672\" xlink:href=\"#ArialMT-53\"/>\n",
              "       <use x=\"194.628906\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_7\">\n",
              "     <g id=\"line2d_13\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 59.339063 73.892066 \n",
              "L 394.139063 73.892066 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_14\">\n",
              "      <!-- 0.775 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(22.315 77.828863)scale(0.11 -0.11)\">\n",
              "       <use xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
              "       <use x=\"83.398438\" xlink:href=\"#ArialMT-55\"/>\n",
              "       <use x=\"139.013672\" xlink:href=\"#ArialMT-55\"/>\n",
              "       <use x=\"194.628906\" xlink:href=\"#ArialMT-53\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"ytick_8\">\n",
              "     <g id=\"line2d_14\">\n",
              "      <path clip-path=\"url(#pa16a309c52)\" d=\"M 59.339063 47.233578 \n",
              "L 394.139063 47.233578 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
              "     </g>\n",
              "     <g id=\"text_15\">\n",
              "      <!-- 0.800 -->\n",
              "      <g style=\"fill:#262626;\" transform=\"translate(22.315 51.170375)scale(0.11 -0.11)\">\n",
              "       <defs>\n",
              "        <path d=\"M 17.671875 38.8125 \n",
              "Q 12.203125 40.828125 9.5625 44.53125 \n",
              "Q 6.9375 48.25 6.9375 53.421875 \n",
              "Q 6.9375 61.234375 12.546875 66.546875 \n",
              "Q 18.171875 71.875 27.484375 71.875 \n",
              "Q 36.859375 71.875 42.578125 66.421875 \n",
              "Q 48.296875 60.984375 48.296875 53.171875 \n",
              "Q 48.296875 48.1875 45.671875 44.5 \n",
              "Q 43.0625 40.828125 37.75 38.8125 \n",
              "Q 44.34375 36.671875 47.78125 31.875 \n",
              "Q 51.21875 27.09375 51.21875 20.453125 \n",
              "Q 51.21875 11.28125 44.71875 5.03125 \n",
              "Q 38.234375 -1.21875 27.640625 -1.21875 \n",
              "Q 17.046875 -1.21875 10.546875 5.046875 \n",
              "Q 4.046875 11.328125 4.046875 20.703125 \n",
              "Q 4.046875 27.6875 7.59375 32.390625 \n",
              "Q 11.140625 37.109375 17.671875 38.8125 \n",
              "z\n",
              "M 15.921875 53.71875 \n",
              "Q 15.921875 48.640625 19.1875 45.40625 \n",
              "Q 22.46875 42.1875 27.6875 42.1875 \n",
              "Q 32.765625 42.1875 36.015625 45.375 \n",
              "Q 39.265625 48.578125 39.265625 53.21875 \n",
              "Q 39.265625 58.0625 35.90625 61.359375 \n",
              "Q 32.5625 64.65625 27.59375 64.65625 \n",
              "Q 22.5625 64.65625 19.234375 61.421875 \n",
              "Q 15.921875 58.203125 15.921875 53.71875 \n",
              "z\n",
              "M 13.09375 20.65625 \n",
              "Q 13.09375 16.890625 14.875 13.375 \n",
              "Q 16.65625 9.859375 20.171875 7.921875 \n",
              "Q 23.6875 6 27.734375 6 \n",
              "Q 34.03125 6 38.125 10.046875 \n",
              "Q 42.234375 14.109375 42.234375 20.359375 \n",
              "Q 42.234375 26.703125 38.015625 30.859375 \n",
              "Q 33.796875 35.015625 27.4375 35.015625 \n",
              "Q 21.234375 35.015625 17.15625 30.90625 \n",
              "Q 13.09375 26.8125 13.09375 20.65625 \n",
              "z\n",
              "\" id=\"ArialMT-56\"/>\n",
              "       </defs>\n",
              "       <use xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"55.615234\" xlink:href=\"#ArialMT-46\"/>\n",
              "       <use x=\"83.398438\" xlink:href=\"#ArialMT-56\"/>\n",
              "       <use x=\"139.013672\" xlink:href=\"#ArialMT-48\"/>\n",
              "       <use x=\"194.628906\" xlink:href=\"#ArialMT-48\"/>\n",
              "      </g>\n",
              "     </g>\n",
              "    </g>\n",
              "    <g id=\"text_16\">\n",
              "     <!-- Test accuracy -->\n",
              "     <g style=\"fill:#262626;\" transform=\"translate(15.789375 168.615937)rotate(-90)scale(0.12 -0.12)\">\n",
              "      <defs>\n",
              "       <path d=\"M 25.921875 0 \n",
              "L 25.921875 63.140625 \n",
              "L 2.34375 63.140625 \n",
              "L 2.34375 71.578125 \n",
              "L 59.078125 71.578125 \n",
              "L 59.078125 63.140625 \n",
              "L 35.40625 63.140625 \n",
              "L 35.40625 0 \n",
              "z\n",
              "\" id=\"ArialMT-84\"/>\n",
              "       <path d=\"M 25.78125 7.859375 \n",
              "L 27.046875 0.09375 \n",
              "Q 23.34375 -0.6875 20.40625 -0.6875 \n",
              "Q 15.625 -0.6875 12.984375 0.828125 \n",
              "Q 10.359375 2.34375 9.28125 4.8125 \n",
              "Q 8.203125 7.28125 8.203125 15.1875 \n",
              "L 8.203125 45.015625 \n",
              "L 1.765625 45.015625 \n",
              "L 1.765625 51.859375 \n",
              "L 8.203125 51.859375 \n",
              "L 8.203125 64.703125 \n",
              "L 16.9375 69.96875 \n",
              "L 16.9375 51.859375 \n",
              "L 25.78125 51.859375 \n",
              "L 25.78125 45.015625 \n",
              "L 16.9375 45.015625 \n",
              "L 16.9375 14.703125 \n",
              "Q 16.9375 10.9375 17.40625 9.859375 \n",
              "Q 17.875 8.796875 18.921875 8.15625 \n",
              "Q 19.96875 7.515625 21.921875 7.515625 \n",
              "Q 23.390625 7.515625 25.78125 7.859375 \n",
              "z\n",
              "\" id=\"ArialMT-116\"/>\n",
              "       <path d=\"M 6.203125 -19.96875 \n",
              "L 5.21875 -11.71875 \n",
              "Q 8.109375 -12.5 10.25 -12.5 \n",
              "Q 13.1875 -12.5 14.9375 -11.515625 \n",
              "Q 16.703125 -10.546875 17.828125 -8.796875 \n",
              "Q 18.65625 -7.46875 20.515625 -2.25 \n",
              "Q 20.75 -1.515625 21.296875 -0.09375 \n",
              "L 1.609375 51.859375 \n",
              "L 11.078125 51.859375 \n",
              "L 21.875 21.828125 \n",
              "Q 23.96875 16.109375 25.640625 9.8125 \n",
              "Q 27.15625 15.875 29.25 21.625 \n",
              "L 40.328125 51.859375 \n",
              "L 49.125 51.859375 \n",
              "L 29.390625 -0.875 \n",
              "Q 26.21875 -9.421875 24.46875 -12.640625 \n",
              "Q 22.125 -17 19.09375 -19.015625 \n",
              "Q 16.0625 -21.046875 11.859375 -21.046875 \n",
              "Q 9.328125 -21.046875 6.203125 -19.96875 \n",
              "z\n",
              "\" id=\"ArialMT-121\"/>\n",
              "      </defs>\n",
              "      <use xlink:href=\"#ArialMT-84\"/>\n",
              "      <use x=\"49.958984\" xlink:href=\"#ArialMT-101\"/>\n",
              "      <use x=\"105.574219\" xlink:href=\"#ArialMT-115\"/>\n",
              "      <use x=\"155.574219\" xlink:href=\"#ArialMT-116\"/>\n",
              "      <use x=\"183.357422\" xlink:href=\"#ArialMT-32\"/>\n",
              "      <use x=\"211.140625\" xlink:href=\"#ArialMT-97\"/>\n",
              "      <use x=\"266.755859\" xlink:href=\"#ArialMT-99\"/>\n",
              "      <use x=\"316.755859\" xlink:href=\"#ArialMT-99\"/>\n",
              "      <use x=\"366.755859\" xlink:href=\"#ArialMT-117\"/>\n",
              "      <use x=\"422.371094\" xlink:href=\"#ArialMT-114\"/>\n",
              "      <use x=\"455.671875\" xlink:href=\"#ArialMT-97\"/>\n",
              "      <use x=\"511.287109\" xlink:href=\"#ArialMT-99\"/>\n",
              "      <use x=\"561.287109\" xlink:href=\"#ArialMT-121\"/>\n",
              "     </g>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"line2d_15\">\n",
              "    <path clip-path=\"url(#pa16a309c52)\" d=\"M 74.557244 230.777301 \n",
              "L 128.485556 168.796296 \n",
              "L 199.774907 106.548725 \n",
              "L 253.703218 77.091069 \n",
              "L 307.63153 57.230537 \n",
              "L 378.920881 33.104574 \n",
              "\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
              "    <defs>\n",
              "     <path d=\"M 0 -8 \n",
              "L -1.796112 -2.472136 \n",
              "L -7.608452 -2.472136 \n",
              "L -2.90617 0.944272 \n",
              "L -4.702282 6.472136 \n",
              "L -0 3.055728 \n",
              "L 4.702282 6.472136 \n",
              "L 2.90617 0.944272 \n",
              "L 7.608452 -2.472136 \n",
              "L 1.796112 -2.472136 \n",
              "z\n",
              "\" id=\"mb4224096f5\" style=\"stroke:#000000;stroke-linejoin:bevel;\"/>\n",
              "    </defs>\n",
              "    <g clip-path=\"url(#pa16a309c52)\">\n",
              "     <use style=\"fill:#ccb974;stroke:#000000;stroke-linejoin:bevel;\" x=\"74.557244\" xlink:href=\"#mb4224096f5\" y=\"230.777301\"/>\n",
              "     <use style=\"fill:#ccb974;stroke:#000000;stroke-linejoin:bevel;\" x=\"128.485556\" xlink:href=\"#mb4224096f5\" y=\"168.796296\"/>\n",
              "     <use style=\"fill:#ccb974;stroke:#000000;stroke-linejoin:bevel;\" x=\"199.774907\" xlink:href=\"#mb4224096f5\" y=\"106.548725\"/>\n",
              "     <use style=\"fill:#ccb974;stroke:#000000;stroke-linejoin:bevel;\" x=\"253.703218\" xlink:href=\"#mb4224096f5\" y=\"77.091069\"/>\n",
              "     <use style=\"fill:#ccb974;stroke:#000000;stroke-linejoin:bevel;\" x=\"307.63153\" xlink:href=\"#mb4224096f5\" y=\"57.230537\"/>\n",
              "     <use style=\"fill:#ccb974;stroke:#000000;stroke-linejoin:bevel;\" x=\"378.920881\" xlink:href=\"#mb4224096f5\" y=\"33.104574\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "   <g id=\"patch_3\">\n",
              "    <path d=\"M 59.339063 240.660937 \n",
              "L 59.339063 23.220937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_4\">\n",
              "    <path d=\"M 394.139063 240.660937 \n",
              "L 394.139063 23.220937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_5\">\n",
              "    <path d=\"M 59.339063 240.660937 \n",
              "L 394.139063 240.660937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
              "   </g>\n",
              "   <g id=\"patch_6\">\n",
              "    <path d=\"M 59.339063 23.220937 \n",
              "L 394.139063 23.220937 \n",
              "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n",
              "   </g>\n",
              "   <g id=\"text_17\">\n",
              "    <!-- STL10 classification over dataset size -->\n",
              "    <g style=\"fill:#262626;\" transform=\"translate(109.632344 17.220937)scale(0.14 -0.14)\">\n",
              "     <defs>\n",
              "      <path d=\"M 4.5 23 \n",
              "L 13.421875 23.78125 \n",
              "Q 14.0625 18.40625 16.375 14.96875 \n",
              "Q 18.703125 11.53125 23.578125 9.40625 \n",
              "Q 28.46875 7.28125 34.578125 7.28125 \n",
              "Q 39.984375 7.28125 44.140625 8.890625 \n",
              "Q 48.296875 10.5 50.3125 13.296875 \n",
              "Q 52.34375 16.109375 52.34375 19.4375 \n",
              "Q 52.34375 22.796875 50.390625 25.3125 \n",
              "Q 48.4375 27.828125 43.953125 29.546875 \n",
              "Q 41.0625 30.671875 31.203125 33.03125 \n",
              "Q 21.34375 35.40625 17.390625 37.5 \n",
              "Q 12.25 40.1875 9.734375 44.15625 \n",
              "Q 7.234375 48.140625 7.234375 53.078125 \n",
              "Q 7.234375 58.5 10.296875 63.203125 \n",
              "Q 13.375 67.921875 19.28125 70.359375 \n",
              "Q 25.203125 72.796875 32.421875 72.796875 \n",
              "Q 40.375 72.796875 46.453125 70.234375 \n",
              "Q 52.546875 67.671875 55.8125 62.6875 \n",
              "Q 59.078125 57.71875 59.328125 51.421875 \n",
              "L 50.25 50.734375 \n",
              "Q 49.515625 57.515625 45.28125 60.984375 \n",
              "Q 41.0625 64.453125 32.8125 64.453125 \n",
              "Q 24.21875 64.453125 20.28125 61.296875 \n",
              "Q 16.359375 58.15625 16.359375 53.71875 \n",
              "Q 16.359375 49.859375 19.140625 47.359375 \n",
              "Q 21.875 44.875 33.421875 42.265625 \n",
              "Q 44.96875 39.65625 49.265625 37.703125 \n",
              "Q 55.515625 34.8125 58.484375 30.390625 \n",
              "Q 61.46875 25.984375 61.46875 20.21875 \n",
              "Q 61.46875 14.5 58.203125 9.4375 \n",
              "Q 54.9375 4.390625 48.796875 1.578125 \n",
              "Q 42.671875 -1.21875 35.015625 -1.21875 \n",
              "Q 25.296875 -1.21875 18.71875 1.609375 \n",
              "Q 12.15625 4.4375 8.421875 10.125 \n",
              "Q 4.6875 15.828125 4.5 23 \n",
              "z\n",
              "\" id=\"ArialMT-83\"/>\n",
              "      <path d=\"M 7.328125 0 \n",
              "L 7.328125 71.578125 \n",
              "L 16.796875 71.578125 \n",
              "L 16.796875 8.453125 \n",
              "L 52.046875 8.453125 \n",
              "L 52.046875 0 \n",
              "z\n",
              "\" id=\"ArialMT-76\"/>\n",
              "      <path d=\"M 6.59375 0 \n",
              "L 6.59375 51.859375 \n",
              "L 14.5 51.859375 \n",
              "L 14.5 44.484375 \n",
              "Q 20.21875 53.03125 31 53.03125 \n",
              "Q 35.6875 53.03125 39.625 51.34375 \n",
              "Q 43.5625 49.65625 45.515625 46.921875 \n",
              "Q 47.46875 44.1875 48.25 40.4375 \n",
              "Q 48.734375 37.984375 48.734375 31.890625 \n",
              "L 48.734375 0 \n",
              "L 39.9375 0 \n",
              "L 39.9375 31.546875 \n",
              "Q 39.9375 36.921875 38.90625 39.578125 \n",
              "Q 37.890625 42.234375 35.28125 43.8125 \n",
              "Q 32.671875 45.40625 29.15625 45.40625 \n",
              "Q 23.53125 45.40625 19.453125 41.84375 \n",
              "Q 15.375 38.28125 15.375 28.328125 \n",
              "L 15.375 0 \n",
              "z\n",
              "\" id=\"ArialMT-110\"/>\n",
              "      <path d=\"M 21 0 \n",
              "L 1.265625 51.859375 \n",
              "L 10.546875 51.859375 \n",
              "L 21.6875 20.796875 \n",
              "Q 23.484375 15.765625 25 10.359375 \n",
              "Q 26.171875 14.453125 28.265625 20.21875 \n",
              "L 39.796875 51.859375 \n",
              "L 48.828125 51.859375 \n",
              "L 29.203125 0 \n",
              "z\n",
              "\" id=\"ArialMT-118\"/>\n",
              "      <path d=\"M 40.234375 0 \n",
              "L 40.234375 6.546875 \n",
              "Q 35.296875 -1.171875 25.734375 -1.171875 \n",
              "Q 19.53125 -1.171875 14.328125 2.25 \n",
              "Q 9.125 5.671875 6.265625 11.796875 \n",
              "Q 3.421875 17.921875 3.421875 25.875 \n",
              "Q 3.421875 33.640625 6 39.96875 \n",
              "Q 8.59375 46.296875 13.765625 49.65625 \n",
              "Q 18.953125 53.03125 25.34375 53.03125 \n",
              "Q 30.03125 53.03125 33.6875 51.046875 \n",
              "Q 37.359375 49.078125 39.65625 45.90625 \n",
              "L 39.65625 71.578125 \n",
              "L 48.390625 71.578125 \n",
              "L 48.390625 0 \n",
              "z\n",
              "M 12.453125 25.875 \n",
              "Q 12.453125 15.921875 16.640625 10.984375 \n",
              "Q 20.84375 6.0625 26.5625 6.0625 \n",
              "Q 32.328125 6.0625 36.34375 10.765625 \n",
              "Q 40.375 15.484375 40.375 25.140625 \n",
              "Q 40.375 35.796875 36.265625 40.765625 \n",
              "Q 32.171875 45.75 26.171875 45.75 \n",
              "Q 20.3125 45.75 16.375 40.96875 \n",
              "Q 12.453125 36.1875 12.453125 25.875 \n",
              "z\n",
              "\" id=\"ArialMT-100\"/>\n",
              "      <path d=\"M 1.953125 0 \n",
              "L 1.953125 7.125 \n",
              "L 34.96875 45.015625 \n",
              "Q 29.34375 44.734375 25.046875 44.734375 \n",
              "L 3.90625 44.734375 \n",
              "L 3.90625 51.859375 \n",
              "L 46.296875 51.859375 \n",
              "L 46.296875 46.046875 \n",
              "L 18.21875 13.140625 \n",
              "L 12.796875 7.125 \n",
              "Q 18.703125 7.5625 23.875 7.5625 \n",
              "L 47.859375 7.5625 \n",
              "L 47.859375 0 \n",
              "z\n",
              "\" id=\"ArialMT-122\"/>\n",
              "     </defs>\n",
              "     <use xlink:href=\"#ArialMT-83\"/>\n",
              "     <use x=\"66.699219\" xlink:href=\"#ArialMT-84\"/>\n",
              "     <use x=\"127.783203\" xlink:href=\"#ArialMT-76\"/>\n",
              "     <use x=\"183.398438\" xlink:href=\"#ArialMT-49\"/>\n",
              "     <use x=\"239.013672\" xlink:href=\"#ArialMT-48\"/>\n",
              "     <use x=\"294.628906\" xlink:href=\"#ArialMT-32\"/>\n",
              "     <use x=\"322.412109\" xlink:href=\"#ArialMT-99\"/>\n",
              "     <use x=\"372.412109\" xlink:href=\"#ArialMT-108\"/>\n",
              "     <use x=\"394.628906\" xlink:href=\"#ArialMT-97\"/>\n",
              "     <use x=\"450.244141\" xlink:href=\"#ArialMT-115\"/>\n",
              "     <use x=\"500.244141\" xlink:href=\"#ArialMT-115\"/>\n",
              "     <use x=\"550.244141\" xlink:href=\"#ArialMT-105\"/>\n",
              "     <use x=\"572.460938\" xlink:href=\"#ArialMT-102\"/>\n",
              "     <use x=\"600.244141\" xlink:href=\"#ArialMT-105\"/>\n",
              "     <use x=\"622.460938\" xlink:href=\"#ArialMT-99\"/>\n",
              "     <use x=\"672.460938\" xlink:href=\"#ArialMT-97\"/>\n",
              "     <use x=\"728.076172\" xlink:href=\"#ArialMT-116\"/>\n",
              "     <use x=\"755.859375\" xlink:href=\"#ArialMT-105\"/>\n",
              "     <use x=\"778.076172\" xlink:href=\"#ArialMT-111\"/>\n",
              "     <use x=\"833.691406\" xlink:href=\"#ArialMT-110\"/>\n",
              "     <use x=\"889.306641\" xlink:href=\"#ArialMT-32\"/>\n",
              "     <use x=\"917.089844\" xlink:href=\"#ArialMT-111\"/>\n",
              "     <use x=\"972.705078\" xlink:href=\"#ArialMT-118\"/>\n",
              "     <use x=\"1022.705078\" xlink:href=\"#ArialMT-101\"/>\n",
              "     <use x=\"1078.320312\" xlink:href=\"#ArialMT-114\"/>\n",
              "     <use x=\"1111.621094\" xlink:href=\"#ArialMT-32\"/>\n",
              "     <use x=\"1139.404297\" xlink:href=\"#ArialMT-100\"/>\n",
              "     <use x=\"1195.019531\" xlink:href=\"#ArialMT-97\"/>\n",
              "     <use x=\"1250.634766\" xlink:href=\"#ArialMT-116\"/>\n",
              "     <use x=\"1278.417969\" xlink:href=\"#ArialMT-97\"/>\n",
              "     <use x=\"1334.033203\" xlink:href=\"#ArialMT-115\"/>\n",
              "     <use x=\"1384.033203\" xlink:href=\"#ArialMT-101\"/>\n",
              "     <use x=\"1439.648438\" xlink:href=\"#ArialMT-116\"/>\n",
              "     <use x=\"1467.431641\" xlink:href=\"#ArialMT-32\"/>\n",
              "     <use x=\"1495.214844\" xlink:href=\"#ArialMT-115\"/>\n",
              "     <use x=\"1545.214844\" xlink:href=\"#ArialMT-105\"/>\n",
              "     <use x=\"1567.431641\" xlink:href=\"#ArialMT-122\"/>\n",
              "     <use x=\"1617.431641\" xlink:href=\"#ArialMT-101\"/>\n",
              "    </g>\n",
              "   </g>\n",
              "  </g>\n",
              " </g>\n",
              " <defs>\n",
              "  <clipPath id=\"pa16a309c52\">\n",
              "   <rect height=\"217.44\" width=\"334.8\" x=\"59.339063\" y=\"23.220937\"/>\n",
              "  </clipPath>\n",
              " </defs>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy for  10 images per label: 62.79%\n",
            "Test accuracy for  20 images per label: 68.60%\n",
            "Test accuracy for  50 images per label: 74.44%\n",
            "Test accuracy for 100 images per label: 77.20%\n",
            "Test accuracy for 200 images per label: 79.06%\n",
            "Test accuracy for 500 images per label: 81.33%\n"
          ]
        }
      ],
      "source": [
        "dataset_sizes = sorted([k for k in results])\n",
        "test_scores = [results[k][\"test\"] for k in dataset_sizes]\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.plot(dataset_sizes, test_scores, '--', color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16)\n",
        "plt.xscale(\"log\")\n",
        "plt.xticks(dataset_sizes, labels=dataset_sizes)\n",
        "plt.title(\"STL10 classification over dataset size\", fontsize=14)\n",
        "plt.xlabel(\"Number of images per class\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.minorticks_off()\n",
        "plt.show()\n",
        "\n",
        "for k, score in zip(dataset_sizes, test_scores):\n",
        "    print(f'Test accuracy for {k:3d} images per label: {100*score:4.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdXMu8eT6xvu"
      },
      "source": [
        "As one would expect, the classification performance improves the more data we have. However, with only 10 images per class, we can already classify more than 60% of the images correctly. This is quite impressive, considering that the images are also higher dimensional than e.g. CIFAR10. With the full dataset, we achieve an accuracy of 81%. The increase between 50 to 500 images per class might suggest a linear increase in performance with an exponentially larger dataset. However, with even more data, we could also finetune $f(\\cdot)$ in the training process, allowing for the representations to adapt more to the specific classification task given.\n",
        "\n",
        "To set the results above into perspective, we will train the base network, a ResNet-18, on the classification task from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDIWnFTQ6xvu"
      },
      "source": [
        "## Baseline\n",
        "\n",
        "As a baseline to our results above, we will train a standard ResNet-18 with random initialization on the labeled training set of STL10. The results will give us an indication of the advantages that contrastive learning on unlabeled data has compared to using only supervised training. The implementation of the model is straightforward since the ResNet architecture is provided in the torchvision library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agPUSYck6xvu"
      },
      "outputs": [],
      "source": [
        "class ResNet(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, num_classes, lr, weight_decay, max_epochs=100):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = torchvision.models.resnet18(num_classes=num_classes)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(),\n",
        "                                lr=self.hparams.lr,\n",
        "                                weight_decay=self.hparams.weight_decay)\n",
        "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                      milestones=[int(self.hparams.max_epochs*0.7),\n",
        "                                                                  int(self.hparams.max_epochs*0.9)],\n",
        "                                                      gamma=0.1)\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def _calculate_loss(self, batch, mode='train'):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        self.log(mode + '_loss', loss)\n",
        "        self.log(mode + '_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._calculate_loss(batch, mode='train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._calculate_loss(batch, mode='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7TXHaeN6xvu"
      },
      "source": [
        "It is clear that the ResNet easily overfits on the training data since its parameter count is more than 1000 times larger than the dataset size. To make the comparison to the contrastive learning models fair, we apply data augmentations similar to the ones we used before: horizontal flip, crop-and-resize, grayscale, and gaussian blur. Color distortions as before are not used because the color distribution of an image showed to be an important feature for the classification. Hence, we observed no noticeable performance gains when adding color distortions to the set of augmentations. Similarly, we restrict the resizing operation before cropping to the max. 125% of its original resolution, instead of 1250% as done in SimCLR. This is because, for classification, the model needs to recognize the full object, while in contrastive learning, we only want to check whether two patches belong to the same image/object. Hence, the chosen augmentations below are overall weaker than in the contrastive learning case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Up4puS6xvu",
        "outputId": "d6b3c8f9-72e1-4648-f9bc-a8b3986152e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.RandomResizedCrop(size=96, scale=(0.8, 1.0)),\n",
        "                                       transforms.RandomGrayscale(p=0.2),\n",
        "                                       transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 0.5)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.5,), (0.5,))\n",
        "                                       ])\n",
        "\n",
        "train_img_aug_data = STL10(root=DATASET_PATH, split='train', download=True,\n",
        "                           transform=train_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8VM2PlO6xvv"
      },
      "source": [
        "The training function for the ResNet is almost identical to the Logistic Regression setup. Note that we allow the ResNet to perform validation every 2 epochs to also check whether the model overfits strongly in the first iterations or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKFDLtcC6xvv"
      },
      "outputs": [],
      "source": [
        "def train_resnet(batch_size, max_epochs=100, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ResNet\"),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=max_epochs,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
        "                                    LearningRateMonitor(\"epoch\")],\n",
        "                         check_val_every_n_epoch=2)\n",
        "    trainer.logger._default_hp_metric = None\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = data.DataLoader(train_img_aug_data, batch_size=batch_size, shuffle=True,\n",
        "                                   drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "    test_loader = data.DataLoader(test_img_data, batch_size=batch_size, shuffle=False,\n",
        "                                  drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ResNet.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
        "        model = ResNet.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42) # To be reproducable\n",
        "        model = ResNet(**kwargs)\n",
        "        trainer.fit(model, train_loader, test_loader)\n",
        "        model = ResNet.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "    # Test best model on validation set\n",
        "    train_result = trainer.test(model, train_loader, verbose=False)\n",
        "    val_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": val_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRE9DTfD6xvv"
      },
      "source": [
        "Finally, let's train the model and check its results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "31a39a2ecb3342f298f31b6e08bf102a",
            "8fcdce7bf86c4c9780e7c9e01a894e94"
          ]
        },
        "id": "P9uTIjEY6xvv",
        "outputId": "32f7bb10-2305-43ac-a7fc-d7b77bac1a90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found pretrained model at ../saved_models/tutorial17/ResNet.ckpt, loading...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/phillip/anaconda3/envs/dl2020/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31a39a2ecb3342f298f31b6e08bf102a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fcdce7bf86c4c9780e7c9e01a894e94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 99.76%\n",
            "Accuracy on test set: 73.31%\n"
          ]
        }
      ],
      "source": [
        "resnet_model, resnet_result = train_resnet(batch_size=64,\n",
        "                                           num_classes=10,\n",
        "                                           lr=1e-3,\n",
        "                                           weight_decay=2e-4,\n",
        "                                           max_epochs=100)\n",
        "print(f\"Accuracy on training set: {100*resnet_result['train']:4.2f}%\")\n",
        "print(f\"Accuracy on test set: {100*resnet_result['test']:4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qH8f6yL6xvv"
      },
      "source": [
        "The ResNet trained from scratch achieves 73.31% on the test set. This is almost 8% less than the contrastive learning model, and even slightly less than SimCLR achieves with 1/10 of the data. This shows that self-supervised, contrastive learning provides considerable performance gains by leveraging large amounts of unlabeled data when little labeled data is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miq3Ukw86xvv"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have discussed self-supervised contrastive learning and implemented SimCLR as an example method. We have applied it to the STL10 dataset and showed that it can learn generalizable representations that we can use to train simple classification models. With 500 images per label, it achieved an 8% higher accuracy than a similar model solely trained from supervision and performs on par with it when only using a tenth of the labeled data. Our experimental results are limited to a single dataset, but recent works such as [Ting Chen et al.](https://arxiv.org/abs/2006.10029) showed similar trends for larger datasets like ImageNet. Besides the discussed hyperparameters, the size of the model seems to be important in contrastive learning as well. If a lot of unlabeled data is available, larger models can achieve much stronger results and come close to their supervised baselines. Further, there are also approaches for combining contrastive and supervised learning, leading to performance gains beyond supervision (see [Khosla et al.](https://arxiv.org/abs/2004.11362)). Moreover, contrastive learning is not the only approach to self-supervised learning that has come up in the last two years and showed great results. Other methods include distillation-based methods like [BYOL](https://arxiv.org/abs/2006.07733) and redundancy reduction techniques like [Barlow Twins](https://arxiv.org/abs/2103.03230). There is a lot more to explore in the self-supervised domain, and more, impressive steps ahead are to be expected.\n",
        "\n",
        "### References\n",
        "\n",
        "[1] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning (ICML 2020). PMLR. ([link](https://arxiv.org/abs/2002.05709))\n",
        "\n",
        "[2] Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. (2020). Big self-supervised models are strong semi-supervised learners. NeurIPS 2021 ([link](https://arxiv.org/abs/2006.10029)).\n",
        "\n",
        "[3] Oord, A. V. D., Li, Y., and Vinyals, O. (2018). Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. ([link](https://arxiv.org/abs/1807.03748))\n",
        "\n",
        "[4] Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C., Pires, B.A., Guo, Z.D., Azar, M.G. and Piot, B. (2020). Bootstrap your own latent: A new approach to self-supervised learning. NeurIPS 2020 ([link](https://arxiv.org/abs/2006.07733))\n",
        "\n",
        "[5] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C. and Krishnan, D. (2020). Supervised contrastive learning. NeurIPS 2020 ([link](https://arxiv.org/abs/2004.11362))\n",
        "\n",
        "[6] Zbontar, J., Jing, L., Misra, I., LeCun, Y. and Deny, S. (2021). Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning (ICML 2021). ([link](https://arxiv.org/abs/2103.03230))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlPvIpO_6xvv"
      },
      "source": [
        "---\n",
        "\n",
        "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=⭐&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider ⭐-ing our repository.    \n",
        "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=❔&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
